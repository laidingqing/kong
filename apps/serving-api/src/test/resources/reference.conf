# API
sparta.api.host = 0.0.0.0
sparta.api.port = 9090
sparta.api.certificate-file = "/home/user/certifications/stratio.jks"
sparta.api.certificate-password = "stratio"


# ZOOKEEPER
sparta.zookeeper.connectionString = "localhost:2181"
sparta.zookeeper.connectionTimeout = 15000
sparta.zookeeper.sessionTimeout = 60000
sparta.zookeeper.retryAttempts = 5
sparta.zookeeper.retryInterval = 10000


# SPARTA AKKA
sparta.akka.controllerActorInstances = 5


# CONFIG
# The execution modes in Sparta are: local, mesos, yarn and standAlone
sparta.config.executionMode = local

# Driver jar served by Sparta in this location
sparta.config.driverPackageLocation = "driver/target"

# Plugin jar are saved in this dir
sparta.config.pluginPackageLocation = "plugins/target"

# The jar driver location in Sparta: hdfs or provided
sparta.config.driverLocation = provided
sparta.config.driverURI = "http://localhost:9090/driver/sparta-driver.jar"

# The rememberPartitioner for RDD generated in Stateful operations in Spark, like UpdateStateByKey
sparta.config.rememberPartitioner = true

# In cluster mode and production environment is recommended set to true
sparta.config.stopGracefully = true

# Maximun time when await to policy status change when run polices in cluster mode
sparta.config.awaitPolicyChangeStatus = 120s

# All the checkpoint options is possible to set it in the policy

# Example value in cluster mode
# sparta.config.checkpointPath = "/user/stratio/sparta/checkpoint"

# Example value in local mode for Stratio Platform
# sparta.config.checkpointPath = "/var/sds/sparta/checkpoint"

# Example value in local mode for debugging
sparta.config.checkpointPath = "/tmp/sparta/checkpoint"

# Auto delete checkpoint when run policies in cluster mode is necessary HDFS configuration
sparta.config.autoDeleteCheckpoint = false

# FRONTEND
sparta.config.frontend.timeout = 5000

# HDFS
# The hadoop user name could be configured by two ways:
# 1. Using the enviroment variable HADOOP_USER_NAME
# 2. Using the variable hadoopUserName from properties file
# sparta.hdfs.hadoopUserName = root

# If the variable HADOOP_CONF_DIR is not defined, "hdfsMaster" variable and "hdfsPort" are used to
# connect to HDFS cluster in order to upload jars to HDFS (plugins and driver), but the Spark executors and the
# Spark driver need this environment variable defined. In producction environments is recomended use
# HADOOP_CONF_DIR because use HA in Hadoop Namenodes, and omit "hdfsMaster and hdfsPort property"
sparta.hdfs.hdfsMaster = hadoopNameNodeAddress
sparta.hdfs.hdfsPort = 9000

# Configuration to connect to HDFS Kerberized

# The principal name could be configured by three ways:
# 1. Using the enviroment variable SPARTA_PRINCIPAL_NAME
# 2. Using the variables "principalNamePrefix" and "principalNameSuffix" and the enviroment variable HOSTNAME
# 3. Using the variable principalName from properties file
# The principal name used to connect to HDFS securized have the order 1, 2 and finally 3.

# sparta.hdfs.principalName = ""
# sparta.hdfs.principalNamePrefix = ""
# sparta.hdfs.principalNameSuffix = ""

# The keytab path could be configured by two ways:
# 1. Using the enviroment variable SPARTA_KEYTAB_PATH
# 2. Using the variable keytabPath from properties file

# sparta.hdfs.keytabPath = ""
# sparta.hdfs.reloadKeyTabTime = 23m


# Spark Options for DEVELOPMENT environments
sparta.local.spark.app.name = SPARTA
sparta.local.spark.master = "local[*]"
sparta.local.spark.driver.memory = 1G
sparta.local.spark.driver.cores = 1
sparta.local.spark.executor.memory = 1G

# Running concurrent jobs brings down the overall processing time and scheduling delay even if a batch
# takes processing time slightly more than batch interval.
# By default the number of concurrent jobs is 1 which means at a time only 1 job will be active
# and till its not finished,other jobs will be queued up even if the resources are available and idle.
# sparta.local.spark.streaming.concurrentJobs = 1

# Turn this down to prevent long blocking at shutdown
# sparta.local.spark.streaming.gracefulStopTimeout = 100000

# Option necessary when run benchmarks
# sparta.local.spark.metrics.conf = /opt/sds/sparta/benchmark/src/main/resources/metrics.properties

# Use other serializer than default Java Serializer
# sparta.local.spark.serializer = org.apache.spark.serializer.KryoSerializer

# Parquet prevention errors
sparta.local.spark.sql.parquet.binaryAsString = true


# Spark Options for MESOS environments

# If the environment variable SPARK_HOME is defined, the spark submit use the variable
sparta.mesos.sparkHome = ""

# Application Name, is possible to assign with "name" and the property spark.app.name.
# spark.app.name = SPARTA

# Coarse mode is recommended in Streaming Applications over Mesos Clusters. If is assinged false is used the
# Spark fine grained mode
sparta.mesos.spark.mesos.coarse = true

# Cluster or Client. If the user need more than one policy running is necessary use "cluster". Is the same as the
# variable spark.submit.deployMode
sparta.mesos.deployMode = cluster

# In Cluster mode is the SparkMesosDispatcher URI, in Client mode the Mesos Master URI
sparta.mesos.master = "mesos://mesosDispatcherURI"

# Url to kill submissions in Spark
sparta.mesos.killUrl = "/v1/submissions/kill"

# Comma-separated list of local jars to include on the driver and executor classpaths
# sparta.mesos.jars = ""

# Comma-separated list of maven coordinates of jars to include on the driver and executor classpaths. Will search
# the local maven repo, then maven central and any additional remote repositories given by --repositories. The
# format for the coordinates should be groupId:artifactId:version
# sparta.mesos.packages = "com:stratio:package"

# Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in --packages
# to avoid dependency conflicts
# sparta.mesos.exclude-packages = "com:stratio:package"

# Comma-separated list of additional remote repositories to search for the maven coordinates given with --packages
# sparta.mesos.repositories = "hdfs://repo"

# User to impersonate when submitting the application. This argument does not work with --principal / --keytab
# sparta.mesos.proxy-user = username

# Extra Java options to pass to the driver
# sparta.mesos.driver-java-options = ""

# Extra library path entries to pass to the driver
# sparta.mesos.driver-library-path = ""

# Extra class path entries to pass to the driver. Note that jars added with --jars are automatically included in
# the classpath
# sparta.mesos.driver-class-path = ""

# Memory by executor
sparta.mesos.spark.executor.memory = 1G

# Total cores by executors, is possible to assign the varibale spark.cores.max.
# If you donâ€™t set spark.cores.max, the Spark application will reserve all resources offered to it by Mesos
sparta.mesos.totalExecutorCores = 2

# IMPORTANT in SPARK 2.x!! Number of executors in Mesos: spark.cores.max/spark.executor.cores
# Cores by executor only in SPARK 2.x
# sparta.mesos.spark.executor.cores = 1

# (Fine-grained mode only) Number of cores to give each Mesos executor
# sparta.mesos.spark.mesos.mesosExecutor.cores = 1

# Extra amount of cpus to request per task
# sparta.mesos.spark.mesos.extra.cores = 0

# Cores assigned to Spark Driver. Cluster mode only
sparta.mesos.spark.driver.cores = 1

# Memory assigned to Spark Driver.
sparta.mesos.spark.driver.memory = 1G

# Restarts the driver on failure, is the same as the property spark.driver.supervise. Cluster mode only
sparta.mesos.supervise = false

# Running concurrent jobs brings down the overall processing time and scheduling delay even if a batch
# takes processing time slightly more than batch interval.
# By default the number of concurrent jobs is 1 which means at a time only 1 job will be active
# and till its not finished,other jobs will be queued up even if the resources are available and idle.
# sparta.mesos.spark.streaming.concurrentJobs = 1

# Turn this down to prevent long blocking at shutdown
# sparta.mesos.spark.streaming.gracefulStopTimeout = 100000

# Use other serializer than default Java Serializer
# sparta.mesos.spark.serializer = org.apache.spark.serializer.KryoSerializer

# Path to a file from which to load extra properties. If notspecified, this will look for conf/spark-defaults.conf.
# sparta.mesos.propertiesFile = ""

# Run over Mesos Clusters with Docker conatiners
# sparta.mesos.spark.mesos.executor.docker.image=dockerAccount/spark-1.6.2-bin-2.6.0
# spark.mesos.executor.docker.forcePullImage=true

# If Spark in Mesos Slaves are installed in a different path than spark submit installation
# sparta.mesos.spark.executor.home=/spark-1.6.2-bin-2.6.0
# sparta.mesos.spark.executor.uri=/spark-mesosphere-scala211-1.6.2-bin-hadoop2.6.0.tgz

# Available in all execution modes
# sparta.mesos.spark.driver.extraClassPath = ""
# sparta.mesos.spark.driver.extraJavaOptions = ""
# sparta.mesos.spark.driver.extraLibraryPath = ""
# sparta.mesos.spark.jars = ""
# sparta.mesos.spark.files = ""

# Available in client mode
# sparta.mesos.spark.jars.ivy = ""

# Parquet prevention errors
sparta.mesos.spark.sql.parquet.binaryAsString = true


# Spark Options for YARN environments

# If the environment variable SPARK_HOME is defined, the spark submit use the variable
sparta.yarn.sparkHome = ""

# Yarn cluster name
sparta.yarn.master = yarn-cluster

# Cluster or Client. If the user need more than one policy running is necessary use "cluster". Is the same as the
# variable spark.submit.deployMode
sparta.yarn.deployMode = cluster

# Url to kill submissions in Spark
sparta.yarn.killUrl = "/v1/submissions/kill"

# Comma-separated list of local jars to include on the driver and executor classpaths
# sparta.yarn.jars = ""

# Comma-separated list of maven coordinates of jars to include on the driver and executor classpaths. Will search
# the local maven repo, then maven central and any additional remote repositories given by --repositories. The
# format for the coordinates should be groupId:artifactId:version
# sparta.yarn.packages = "com:stratio:package"

# Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in --packages
# to avoid dependency conflicts
# sparta.yarn.exclude-packages = "com:stratio:package"

# Comma-separated list of additional remote repositories to search for the maven coordinates given with --packages
# sparta.yarn.repositories = "hdfs://repo"

# User to impersonate when submitting the application. This argument does not work with --principal / --keytab
# sparta.yarn.proxy-user = username

# Extra Java options to pass to the driver
# sparta.yarn.driver-java-options = ""

# Extra library path entries to pass to the driver
# sparta.yarn.driver-library-path = ""

# Extra class path entries to pass to the driver. Note that jars added with --jars are automatically included in
# the classpath
# sparta.yarn.driver-class-path = ""

# Application Name, is possible to assign with "name" and the property spark.app.name.
# sparta.yarn.name = SPARTA

# Number of executors, is the same as the property spark.executor.instances.
sparta.yarn.numExecutors = 1

# Memory by executor, is the same as the property spark.executor.memory.
sparta.yarn.executorMemory = 1G

# Cores by executor, is possible to assign the varibale spark.executor.cores.
sparta.yarn.executorCores = 1

# Memory assigned to Spark Driver. Cluster mode only.
sparta.yarn.driverMemory = 1G

# Memory assigned to Spark Driver. Client mode only.
# sparta.yarn.spark.driver.memory = 1G

# Cores assigned to Spark Driver, is the same as the property spark.driver.cores. Cluster mode only.
sparta.yarn.driverCores = 1

# The YARN queue to submit to (Default: "default"). Client mode only.
# sparta.yarn.spark.yarn.queue = queuename

# Path to a file from which to load extra properties. If notspecified, this will look for conf/spark-defaults.conf.
# sparta.yarn.propertiesFile = ""

# Comma separated list of archives to be extracted into the working directory of each executor. Client mode only.
# sparta.yarn.spark.yarn.dist.files = ""
# sparta.yarn.spark.yarn.dist.archives = ""

# Comma separated list of archives to be extracted into the working directory of each executor. Cluster mode only.
# sparta.yarn.files = ""
# sparta.yarn.archives = ""

# Comma-separated list of jars
# sparta.yarn.addJars = ""

# Available in all execution modes
# sparta.yarn.spark.driver.extraClassPath = ""
# sparta.yarn.spark.driver.extraJavaOptions = ""
# sparta.yarn.spark.driver.extraLibraryPath = ""

# Available in client mode
# sparta.yarn.spark.jars = ""
# sparta.yarn.spark.jars.ivy = ""

# Parquet prevention errors
sparta.yarn.spark.sql.parquet.binaryAsString = true


# Spark Options for SPARK STANDALONE environments

# If the environment variable SPARK_HOME is defined, the spark submit use the variable
sparta.standalone.sparkHome = ""

# Spark Master URI
sparta.standalone.master = "spark://127.0.0.1:7077"

# Cluster or Client. If the user need more than one policy running is necessary use "cluster". Is the same as the
# variable spark.submit.deployMode
sparta.standalone.deployMode = cluster

# Url to kill submissions in Spark
sparta.standalone.killUrl = "/v1/submissions/kill"

# Comma-separated list of local jars to include on the driver and executor classpaths
# sparta.standalone.jars = ""

# Comma-separated list of maven coordinates of jars to include on the driver and executor classpaths. Will search
# the local maven repo, then maven central and any additional remote repositories given by --repositories. The
# format for the coordinates should be groupId:artifactId:version
# sparta.standalone.packages = "com:stratio:package"

# Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in --packages
# to avoid dependency conflicts
# sparta.standalone.exclude-packages = "com:stratio:package"

# Comma-separated list of additional remote repositories to search for the maven coordinates given with --packages
# sparta.standalone.repositories = "hdfs://repo"

# User to impersonate when submitting the application. This argument does not work with --principal / --keytab
# sparta.standalone.proxy-user = username

# Extra Java options to pass to the driver
# sparta.standalone.driver-java-options = ""

# Extra library path entries to pass to the driver
# sparta.standalone.driver-library-path = ""

# Extra class path entries to pass to the driver. Note that jars added with --jars are automatically included in
# the classpath
# sparta.standalone.driver-class-path = ""

# Application Name, is possible to assign with "name" and the property spark.app.name.
# sparta.standalone.spark.app.name = SPARTA

# Cores assigned to Spark Driver. Cluster mode only
sparta.standalone.spark.driver.cores = 1

# Memory assigned to Spark Driver.
sparta.standalone.spark.driver.memory = 1G

# Cores by executor
sparta.standalone.spark.executor.cores = 1

# Memory by executor
sparta.standalone.spark.executor.memory = 1G

# Total cores by executors, is possible to assign the varibale spark.cores.max
sparta.standalone.totalExecutorCores = 2

# Restarts the driver on failure, is the same as the property spark.driver.supervise. Cluster mode only
sparta.standalone.supervise = false

# Path to a file from which to load extra properties. If notspecified, this will look for conf/spark-defaults.conf.
# sparta.standalone.propertiesFile = ""

# Available in all execution modes
# sparta.standalone.spark.driver.extraClassPath = ""
# sparta.standalone.spark.driver.extraJavaOptions = ""
# sparta.standalone.spark.driver.extraLibraryPath = ""
# sparta.standalone.spark.jars = ""
# sparta.standalone.spark.jars.ivy = ""
# sparta.standalone.spark.files = ""

# Parquet prevention errors
sparta.standalone.spark.sql.parquet.binaryAsString = true


# OAUTH2
oauth2.enable = "false"
oauth2.cookieName = "user"
oauth2.url.authorize = "https://server.domain:9005/cas/oauth2.0/authorize"
oauth2.url.accessToken = "https://server.domain:9005/cas/oauth2.0/accessToken"
oauth2.url.profile = "https://server.domain:9005/cas/oauth2.0/profile"
oauth2.url.logout = "https://server.domain:9005/cas/logout"
oauth2.url.callBack = "http://callback.domain:9090/login"
oauth2.url.onLoginGoTo = "/"
oauth2.client.id = "userid"
oauth2.client.secret = "usersecret"


# SPRAY
spray.can.server.ssl-encryption = "off"


# AKKA
akka.log-dead-letters = off
akka.loggers = ["akka.event.slf4j.Slf4jLogger"]

# AKKA test properties
akka.test.single-expect-default=5000
akka.test.default-timeout=5000
akka.test.timefactor=5