# API

kongcloud.api.host = 0.0.0.0
kongcloud.api.port = 9090
kongcloud.api.certificate-file = "/home/user/certifications/stratio.jks"
kongcloud.api.certificate-password = "stratio"


# ZOOKEEPER

kongcloud.zookeeper.connectionString = "localhost:2181"
kongcloud.zookeeper.connectionTimeout = 15000
kongcloud.zookeeper.sessionTimeout = 60000
kongcloud.zookeeper.retryAttempts = 5
kongcloud.zookeeper.retryInterval = 10000


# OAUTH2

oauth2.enable = "true" //为true时，必须启动security统一认证服务。
oauth2.url.check.token = "http://localhost:9081/oauth/check_token"
oauth2.cookieName = "user"
# SPRAY

spray.can.server.ssl-encryption = "off"


# AKKA

akka.log-dead-letters = off
akka.loggers = ["akka.event.slf4j.Slf4jLogger"]
akka.logger-startup-timeout = 30s


# CONFIG

# The execution modes in Sparta are: local, mesos or marathon
kongcloud.config.executionMode = local

# Driver jar served by Sparta in this location
kongcloud.config.driverPackageLocation = "/tmp/kongcloud/driver"

# Plugin jar are saved in this dir
kongcloud.config.pluginPackageLocation = "/tmp/kongcloud/plugins"

# Backups files are saved in this dir
kongcloud.config.backupsLocation = "/tmp/kongcloud/backups"

# The jar driver location in Sparta: hdfs, http or local
kongcloud.config.driverURI = "http://0.0.0.0:9090/driver/kongcloud-driver.jar"

# The rememberPartitioner for RDD generated in Stateful operations in Spark, like UpdateStateByKey
sparta.config.rememberPartitioner = true

# Maximun time when await to policy status change when run polices in cluster mode
kongcloud.config.awaitPolicyChangeStatus = 180s

# Example value in local mode for debugging
kongcloud.config.checkpointPath = "/tmp/sparta/checkpoint"

# Auto delete checkpoint when run policies in cluster mode is necessary HDFS configuration
kongcloud.config.autoDeleteCheckpoint = true

# Add time to checkpoint path when run policies in cluster mode is necessary HDFS configuration
kongcloud.config.addTimeToCheckpointPath = false

# FRONTEND
kongcloud.config.frontend.timeout = 5000

# HDFS

# The hadoop user name could be configured by two ways:
# 1. Using the enviroment variable HADOOP_USER_NAME
# 2. Using the variable hadoopUserName from properties file
# kongcloud.hdfs.hadoopUserName = root

# If the variable HADOOP_CONF_DIR is not defined, "hdfsMaster" variable and "hdfsPort" are used to
# connect to HDFS cluster in order to upload jars to HDFS (plugins and driver), but the Spark executors and the
# Spark driver need this environment variable defined. In producction environments is recomended use
# HADOOP_CONF_DIR because use HA in Hadoop Namenodes, and omit "hdfsMaster and hdfsPort property"
# kongcloud.hdfs.hdfsMaster = hadoopNameNodeAddress
# kongcloud.hdfs.hdfsPort = 9000

# Configuration to connect to HDFS Kerberized

# The principal name could be configured by two ways:
# 1. Using the enviroment variable SPARTA_PRINCIPAL_NAME
# 2. Using the variable principalName from properties file
# The principal name used to connect to HDFS securized have the order 1, 2
# kongcloud.hdfs.principalName = ""

# The keytab path could be configured by two ways:
# 1. Using the enviroment variable SPARTA_KEYTAB_PATH
# 2. Using the variable keytabPath from properties file

# kongcloud.hdfs.keytabPath = ""
# kongcloud.hdfs.reloadKeyTab = false
# kongcloud.hdfs.reloadKeyTabTime = 23h



### SPARK MESOS deployments ###

# If the environment variable SPARK_HOME is defined, the spark submit use the variable
# kongcloud.mesos.sparkHome = ""

# Application Name, is possible to assign with "name" and the property spark.app.name.
# spark.app.name = KongCloud

# Coarse mode is recommended in Streaming Applications over Mesos Clusters. If is assinged false is used the
# Spark fine grained mode
kongcloud.mesos.spark.mesos.coarse = true

# Cluster or Client. If the user need more than one policy running is necessary use "cluster". Is the same as the
# variable spark.submit.deployMode
kongcloud.mesos.deployMode = cluster

# In Cluster mode is the SparkMesosDispatcher URI(master:7077), in Client mode the Mesos Master URI(master:5050)
# kongcloud.mesos.master = "mesos://mesosURI"

# Url to kill submissions in Spark with Spark Dispatcher API
# kongcloud.mesos.killUrl = "http://mesosDispatcherURL/v1/submissions/kill"

# Principal name used when run Sparta over Mesos securized
# kongcloud.mesos.spark.mesos.principal=dcs1

# Secret password used when run Sparta over Mesos securized
# kongcloud.mesos.spark.mesos.secret=dcs_1_password

# Role assigned to spark jobs when run Sparta over Mesos securized
# kongcloud.mesos.spark.mesos.role=sparta

# Comma-separated list of local jars to include on the driver and executor classpaths
# kongcloud.mesos.jars = ""

# Comma-separated list of maven coordinates of jars to include on the driver and executor classpaths. Will search
# the local maven repo, then maven central and any additional remote repositories given by --repositories. The
# format for the coordinates should be groupId:artifactId:version
# kongcloud.mesos.packages = "com:ckmro:package"

# Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in --packages
# to avoid dependency conflicts
# kongcloud.mesos.exclude-packages = "com:ckmro:package"

# Comma-separated list of additional remote repositories to search for the maven coordinates given with --packages
# kongcloud.mesos.repositories = "hdfs://repo"

# User to impersonate when submitting the application. This argument does not work with --principal / --keytab
# kongcloud.mesos.proxy-user = username

# Extra Java options to pass to the driver
# kongcloud.mesos.driver-java-options = ""

# Extra library path entries to pass to the driver
# kongcloud.mesos.driver-library-path = ""

# Extra class path entries to pass to the driver. Note that jars added with --jars are automatically included in
# the classpath
# kongcloud.mesos.driver-class-path = ""

# Memory by executor
kongcloud.mesos.spark.executor.memory = 1G

# Total cores by executors, is possible to assign the varibale spark.cores.max.
# If you don’t set spark.cores.max, the Spark application will reserve all resources offered to it by Mesos
kongcloud.mesos.totalExecutorCores = 2

# IMPORTANT in SPARK 2.x!! Number of executors in Mesos: spark.cores.max/spark.executor.cores
# Cores by executor only in SPARK 2.x
kongcloud.mesos.spark.executor.cores = 1

# Cores assigned to Spark Driver. Cluster mode only
kongcloud.mesos.spark.driver.cores = 1

# Memory assigned to Spark Driver.
kongcloud.mesos.spark.driver.memory = 1G

# Extra amount of cpus to request per task
# kongcloud.mesos.spark.mesos.extra.cores = 0

# Restarts the driver on failure, is the same as the property spark.driver.supervise. Cluster mode only
# kongcloud.mesos.supervise = false

# Running concurrent jobs brings down the overall processing time and scheduling delay even if a batch
# takes processing time slightly more than batch interval.
# By default the number of concurrent jobs is 1 which means at a time only 1 job will be active
# and till its not finished,other jobs will be queued up even if the resources are available and idle.
# kongcloud.mesos.spark.streaming.concurrentJobs = 1

# Stop gracefully the streaming contexts when shutdown the spark applications
# kongcloud.streaming.stopGracefullyOnShutdown = true

# Turn this down to prevent long blocking at shutdown
# kongcloud.mesos.spark.streaming.gracefulStopTimeout = 100000

# Use other serializer than default Java Serializer
# kongcloud.mesos.spark.serializer = org.apache.spark.serializer.KryoSerializer

# Path to a file from which to load extra properties. If notspecified, this will look for conf/spark-defaults.conf.
# kongcloud.mesos.propertiesFile = ""

# Run over Mesos Clusters with Docker conatiners
# kongcloud.mesos.spark.mesos.executor.docker.image="stratio/mesos-spark-2.1-scala-2.11-executor-dcos"

# Force pull image for each execution
# kongcloud.mesos.spark.mesos.executor.docker.forcePullImage=true

# Mount volumes from host machine in the executor docker container
# kongcloud.mesos.spark.mesos.executor.docker.volumes="/opt/mesosphere/packages/:/opt/mesosphere/packages/:ro,/opt/mesosphere/lib/:/opt/mesosphere/lib/:ro"

# Send environment variables to executor docker containers only in Spark slaves
# sparta.mesos.spark.executorEnv.MESOS_NATIVE_JAVA_LIBRARY="/opt/mesosphere/lib/libmesos.so"

# If Spark in Mesos Slaves are installed in a different path than spark submit installation
# kongcloud.mesos.spark.mesos.executor.home=/opt/spark/dist

# If you want to use one Spark compilation
# kongcloud.mesos.spark.executor.uri="http://tools.stratio.com/spark/spark-mesosphere-scala211-1.6.2-bin-hadoop2.6.0.tgz"

# A comma-separated list of URIs to be downloaded to the sandbox when driver or executor is launched by Mesos
# kongcloud.mesos.spark.mesos.uris="http://hadoopNamenode:8020/conf/core-site.xml"

# Download HDFS conf in stratio spark fork
# kongcloud.mesos.spark.mesos.driverEnv.HDFS_CONF_URI=/opt/sds/hadoop/conf

# Increase job parallelity by reducing Spark Delay Scheduling (potentially big performance impact (!)) (Default: 3s)
# kongcloud.mesos.spark.locality.wait=10

# Increase max task failures before failing job (Default: 4)
# kongcloud.mesos.spark.task.maxFailures=8

# Tweak to balance data processing parallelism vs. task scheduling overhead (Default: 200ms)
# kongcloud.mesos.spark.streaming.blockInterval=200

# Available in all execution modes
# kongcloud.mesos.spark.driver.extraClassPath = ""
# kongcloud.mesos.spark.driver.extraJavaOptions = ""
# kongcloud.mesos.spark.driver.extraLibraryPath = ""
# kongcloud.mesos.spark.jars = ""
# kongcloud.mesos.spark.files = ""

# Available in client mode
# kongcloud.mesos.spark.jars.ivy = ""

# Parquet prevention errors
# kongcloud.mesos.spark.sql.parquet.binaryAsString = true

# How long to wait to launch a data-local task before giving up and launching it on a less-local node.
#kongcloud.mesos.spark.locality.wait = 10s

# Number of failures of any particular task before giving up on the job.
#kongcloud.mesos.spark.task.maxFailures = 8

# Interval at which data received by Spark Streaming receivers is chunked into blocks of data before storing them in Spark
#kongcloud.mesos.spark.streaming.blockInterval = 200ms

# Spark User in dispatcher deployments
# kongcloud.mesos.spark.mesos.driverEnv.SPARK_USER="sparta"


### SPARK MARATHON deployments ###

kongcloud.marathon.docker.image = "qa.stratio.com/stratio/sparta:1.4.0-SNAPSHOT"
kongcloud.marathon.docker.forcePullImage = false
kongcloud.marathon.docker.privileged = false
kongcloud.marathon.jar = "/opt/sds/sparta/driver/sparta-driver.jar"
kongcloud.marathon.template.file = "/etc/sds/sparta/marathon-app-template.json"
kongcloud.marathon.mesosphere.lib = "/opt/mesosphere/lib"
kongcloud.marathon.mesosphere.packages = "/opt/mesosphere/packages"
kongcloud.marathon.gracePeriodSeconds = 180
kongcloud.marathon.intervalSeconds = 60
kongcloud.marathon.timeoutSeconds = 20
kongcloud.marathon.maxConsecutiveFailures = 3
# sparta.marathon.sso.uri = "https://gosec2.labs.stratio.com:9005/gosec-sso"
# sparta.marathon.sso.username = "admin"
# sparta.marathon.sso.password = "1234"
# sparta.marathon.sso.clientId = "adminrouter_paas-master-1.labs.stratio.com"
# sparta.marathon.sso.redirectUri = "https://master-1.labs.stratio.com/acs/api/v1/auth/login&firstUser=false"
# sparta.marathon.tikitakka.marathon.uri = "http://master-1.labs.stratio.com:8080"
kongcloud.marathon.tikitakka.marathon.api.version = "v2"


### SPARK LOCAL deployments ###

kongcloud.local.spark.app.name = KongCloud
kongcloud.local.spark.master = "local[*]"
kongcloud.local.spark.driver.memory = 1G
kongcloud.local.spark.executor.memory = 1G
# sparta.local.spark.metrics.conf = /opt/sds/sparta/benchmark/src/main/resources/metrics.properties

### Kong Cloud DbStore for Mongodb

kongcloud.mongo.uri = "mongodb://localhost:27017"
kongcloud.mongo.dbName = "sparta"